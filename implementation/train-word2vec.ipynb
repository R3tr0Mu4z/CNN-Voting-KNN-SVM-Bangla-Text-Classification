{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f933f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Dataset 1 (Bangla Wikipedia Dataset) : https://www.kaggle.com/datasets/saurabhshahane/bangla-wikipedia\n",
    "    Dataset 2 (Bangla ( Bengali ) sentiment analysis classification benchmark dataset corpus) : https://data.mendeley.com/datasets/p6zc7krs37/4\n",
    "    Dataset 3 (Bangla Online Comments Dataset) : https://data.mendeley.com/datasets/9xjx8twk8p/1\n",
    "\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8966bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70377\n",
      "44001\n",
      "11807\n"
     ]
    }
   ],
   "source": [
    "# Loading Bangla Wiki dataset\n",
    "df = pd.read_csv('../datasets/wiki.csv')\n",
    "\n",
    "\n",
    "wiki = df['text'].tolist()\n",
    "\n",
    "df = pd.read_excel('../datasets/bangla_online_comments_dataset.xlsx')\n",
    "\n",
    "comments1 = df['comment'].tolist()\n",
    "\n",
    "\n",
    "# Loading Bangla Sentiment analysis comments\n",
    "positive_sentences = []\n",
    "f = open('../datasets/all_positive_8500.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    positive_sentences.append(line.strip())\n",
    "\n",
    "negative_sentences = []\n",
    "f = open('../datasets/all_negative_3307.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    negative_sentences.append(line.strip())\n",
    "    \n",
    "comments2 = positive_sentences + negative_sentences\n",
    "\n",
    "print(len(wiki))\n",
    "print(len(comments1))\n",
    "print(len(comments2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e53f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the dataset\n",
    "# Codes used from https://www.kaggle.com/code/sadmanaraf/bangla-embedding-training-using-gensim/notebook\n",
    "def remove_punc(sentences):\n",
    "    new_sentences=[]\n",
    "    exclude = list(set(string.punctuation))\n",
    "    exclude.extend([\"’\", \"‘\", \"—\",\"\\n\",'০','১','২','৩','৪','৫','৬','৭','৮','৯'])\n",
    "    for sentence in sentences:\n",
    "        s = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "        new_sentences.append(s)\n",
    "    \n",
    "    return new_sentences\n",
    "\n",
    "def sentences_cleaner(sentences):\n",
    "    clean_sentences = remove_punc(sentences)\n",
    "    clean_sentences = [article.split('।') for article in clean_sentences]\n",
    "    clean_sentences = [item for sublist in clean_sentences for item in sublist]\n",
    "    clean_sentences = [item.strip() for item in clean_sentences if len(item.split())>1]\n",
    "    clean_sentences = [item.split() for item in clean_sentences]\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0c24841",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences_cleaner(wiki + comments1 + comments2) , vector_size=300, window=5, min_count=3)\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aaf1f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('আর্জেন্টিনা', 0.9085292816162109),\n",
       " ('চিলি', 0.8727936148643494),\n",
       " ('স্পেন', 0.8717064261436462),\n",
       " ('উরুগুয়ে', 0.8570982217788696),\n",
       " ('বেলজিয়াম', 0.8558390140533447),\n",
       " ('পর্তুগাল', 0.8468490242958069),\n",
       " ('নেদারল্যান্ডস', 0.8430261611938477),\n",
       " ('ইতালি', 0.8414232134819031),\n",
       " ('কেনিয়া', 0.8292302489280701),\n",
       " ('আয়ারল্যান্ড', 0.8041154742240906)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('ব্রাজিল')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bc618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
